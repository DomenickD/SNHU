{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# If score_logger is not available, you can remove it or implement a simple logging mechanism.\n",
    "from scores.score_logger import ScoreLogger\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995\n",
    "\n",
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.array(state)  # Ensure state is a NumPy array\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        state = np.reshape(state, [1, -1])  # Reshape state to match model input\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            state = np.array(state)  # Ensure state is a NumPy array\n",
    "            state_next = np.array(state_next)  # Ensure next state is a NumPy array\n",
    "            state = np.reshape(state, [1, -1])\n",
    "            state_next = np.reshape(state_next, [1, -1])\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = reward + GAMMA * np.amax(\n",
    "                    self.model.predict(state_next, verbose=0)[0]\n",
    "                )\n",
    "            q_values = self.model.predict(state, verbose=0)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "def cartpole():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n\n",
    "    dqn_solver = DQNSolver(observation_space, action_space)\n",
    "    run = 0\n",
    "    while run <= 100:\n",
    "        run += 1\n",
    "        state, _ = env.reset()  # Unpack the tuple and ignore the dictionary\n",
    "        state = np.array(state)  # Convert only the relevant part to a NumPy array\n",
    "        state = np.reshape(state, [1, -1])  # Correctly reshape the initial state\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = dqn_solver.act(state)\n",
    "            results = env.step(action)\n",
    "            state_next = results[0]  # Extract the state_next from the results\n",
    "            reward = results[1]\n",
    "            terminal = results[2]\n",
    "            # Handle any additional returned values (like info) as necessary\n",
    "            reward = reward if not terminal else -reward\n",
    "            state_next = np.array(state_next)  # Ensure next state is a NumPy array\n",
    "            state_next = np.reshape(\n",
    "                state_next, [1, -1]\n",
    "            )  # Correctly reshape the next state\n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                print(\n",
    "                    f\"Run: {run}, exploration: {dqn_solver.exploration_rate:.4f}, score: {step}\"\n",
    "                )\n",
    "                break\n",
    "            dqn_solver.experience_replay()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Domenick Dobbs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\Domenick Dobbs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 0.8911, score: 43\n",
      "Run: 2, exploration: 0.8518, score: 10\n",
      "Run: 3, exploration: 0.7862, score: 17\n",
      "Run: 4, exploration: 0.7292, score: 16\n",
      "Run: 5, exploration: 0.6798, score: 15\n",
      "Run: 6, exploration: 0.6433, score: 12\n",
      "Run: 7, exploration: 0.5997, score: 15\n",
      "Run: 8, exploration: 0.5676, score: 12\n",
      "Run: 9, exploration: 0.5186, score: 19\n",
      "Run: 10, exploration: 0.4908, score: 12\n",
      "Run: 11, exploration: 0.4621, score: 13\n",
      "Run: 12, exploration: 0.4308, score: 15\n",
      "Run: 13, exploration: 0.4057, score: 13\n",
      "Run: 14, exploration: 0.3839, score: 12\n",
      "Run: 15, exploration: 0.3670, score: 10\n",
      "Run: 16, exploration: 0.3455, score: 13\n",
      "Run: 17, exploration: 0.3110, score: 22\n",
      "Run: 18, exploration: 0.2973, score: 10\n",
      "Run: 19, exploration: 0.2799, score: 13\n",
      "Run: 20, exploration: 0.2649, score: 12\n",
      "Run: 21, exploration: 0.2445, score: 17\n",
      "Run: 22, exploration: 0.2326, score: 11\n",
      "Run: 23, exploration: 0.2212, score: 11\n",
      "Run: 24, exploration: 0.2072, score: 14\n",
      "Run: 25, exploration: 0.1810, score: 28\n",
      "Run: 26, exploration: 0.1333, score: 62\n",
      "Run: 27, exploration: 0.1075, score: 44\n",
      "Run: 28, exploration: 0.0772, score: 67\n",
      "Run: 29, exploration: 0.0546, score: 70\n",
      "Run: 30, exploration: 0.0458, score: 36\n",
      "Run: 31, exploration: 0.0318, score: 74\n",
      "Run: 32, exploration: 0.0256, score: 44\n",
      "Run: 33, exploration: 0.0170, score: 83\n",
      "Run: 34, exploration: 0.0104, score: 98\n",
      "Run: 35, exploration: 0.0100, score: 63\n",
      "Run: 36, exploration: 0.0100, score: 60\n",
      "Run: 37, exploration: 0.0100, score: 44\n",
      "Run: 38, exploration: 0.0100, score: 61\n",
      "Run: 39, exploration: 0.0100, score: 56\n",
      "Run: 40, exploration: 0.0100, score: 122\n",
      "Run: 41, exploration: 0.0100, score: 67\n",
      "Run: 42, exploration: 0.0100, score: 72\n",
      "Run: 43, exploration: 0.0100, score: 106\n",
      "Run: 44, exploration: 0.0100, score: 305\n",
      "Run: 45, exploration: 0.0100, score: 107\n",
      "Run: 46, exploration: 0.0100, score: 188\n",
      "Run: 47, exploration: 0.0100, score: 233\n",
      "Run: 48, exploration: 0.0100, score: 146\n",
      "Run: 49, exploration: 0.0100, score: 242\n",
      "Run: 50, exploration: 0.0100, score: 214\n",
      "Run: 51, exploration: 0.0100, score: 217\n",
      "Run: 52, exploration: 0.0100, score: 214\n",
      "Run: 53, exploration: 0.0100, score: 228\n",
      "Run: 54, exploration: 0.0100, score: 222\n",
      "Run: 55, exploration: 0.0100, score: 252\n",
      "Run: 56, exploration: 0.0100, score: 203\n",
      "Run: 57, exploration: 0.0100, score: 221\n",
      "Run: 58, exploration: 0.0100, score: 219\n",
      "Run: 59, exploration: 0.0100, score: 196\n",
      "Run: 60, exploration: 0.0100, score: 227\n",
      "Run: 61, exploration: 0.0100, score: 231\n",
      "Run: 62, exploration: 0.0100, score: 217\n",
      "Run: 63, exploration: 0.0100, score: 230\n",
      "Run: 64, exploration: 0.0100, score: 234\n",
      "Run: 65, exploration: 0.0100, score: 210\n",
      "Run: 66, exploration: 0.0100, score: 220\n",
      "Run: 67, exploration: 0.0100, score: 219\n",
      "Run: 68, exploration: 0.0100, score: 247\n",
      "Run: 69, exploration: 0.0100, score: 281\n",
      "Run: 70, exploration: 0.0100, score: 228\n",
      "Run: 71, exploration: 0.0100, score: 230\n",
      "Run: 72, exploration: 0.0100, score: 279\n",
      "Run: 73, exploration: 0.0100, score: 226\n",
      "Run: 74, exploration: 0.0100, score: 248\n",
      "Run: 75, exploration: 0.0100, score: 220\n",
      "Run: 76, exploration: 0.0100, score: 63\n",
      "Run: 77, exploration: 0.0100, score: 209\n",
      "Run: 78, exploration: 0.0100, score: 219\n",
      "Run: 79, exploration: 0.0100, score: 202\n",
      "Run: 80, exploration: 0.0100, score: 202\n",
      "Run: 81, exploration: 0.0100, score: 201\n",
      "Run: 82, exploration: 0.0100, score: 185\n",
      "Run: 83, exploration: 0.0100, score: 205\n",
      "Run: 84, exploration: 0.0100, score: 241\n",
      "Run: 85, exploration: 0.0100, score: 107\n",
      "Run: 86, exploration: 0.0100, score: 212\n",
      "Run: 87, exploration: 0.0100, score: 262\n",
      "Run: 88, exploration: 0.0100, score: 184\n",
      "Run: 89, exploration: 0.0100, score: 222\n",
      "Run: 90, exploration: 0.0100, score: 246\n",
      "Run: 91, exploration: 0.0100, score: 250\n",
      "Run: 92, exploration: 0.0100, score: 187\n",
      "Run: 93, exploration: 0.0100, score: 205\n",
      "Run: 94, exploration: 0.0100, score: 135\n",
      "Run: 95, exploration: 0.0100, score: 123\n",
      "Run: 96, exploration: 0.0100, score: 241\n",
      "Run: 97, exploration: 0.0100, score: 186\n",
      "Run: 98, exploration: 0.0100, score: 216\n",
      "Run: 99, exploration: 0.0100, score: 151\n",
      "Run: 100, exploration: 0.0100, score: 189\n",
      "Run: 101, exploration: 0.0100, score: 198\n"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this agent is to do its very best to keep a pole balanced on a cart for as long as possible. By doing so it will gain the highest score. \n",
    "The state values are the cart position, cart velocity, pole angle, and angualar velocity. \n",
    "The agent only has the ability to move left and right. These are the possible actions that can be performed. \n",
    "The agent is trained using the Deep Q-learning algorithm which is a different version of just the Q-Learning algorithm. The Deep version of this algorithm used a neural network (often referred to as deep learning) in order to approximate the values. We can only approximate as long as the relative importance is preserved (Kumar, 2023).\n",
    "\n",
    "Experience reply is actually really cool. We use a replay buffer that allows the agent to access it's training experience more efficently during it's training! (Hugging Face, (n.d.))\n",
    "The discount factor is how future rewards are weighted relative to the more immediate rewards. SO if it is closer to 1, the agent is going to weight more long term rewards as, more. It is almost alive in how the agent has to balance short term and long term rewards. This could help with my chess example!\n",
    "\n",
    "Neural Network Architecture for this problem; because we are using Deep Q-learning, we are leveraging a neural network. This means we have an input layer that is taking the state vector as input, layered on some hidden layers that must have Relu in there. We have another Dense layer for linear. This is whith just 24 neurons. \n",
    "Using the neural network approach is going to increase efficency accorss similar stateswhich can help with large or continuos spaces as opposed to maintaining one table for every possible action state pair. neural networks help with approximating the more generalized data as I mentioned about after reading geeksforgeeks. \n",
    "\n",
    "Learning rate (which is no longer \"lr\" - I fixed alot of old code that was given), is responsible for finding the best possiblescore on the problem. If it is too large, we see that it is instable which is because it may jump over what we would score as optimal. If it is too low, we get a stable result but it can take much longer. Many Data scientists call the learning rate and art more than a science but there are a few methods for optimizing it. In non reinforcment learning, I like to create a matrix and plot out the results of different learning rates before honing in on an ideal one. Another method is hyperparameter tuning like randomsearchcv by sklearn or gridsearchcv  (if your hardware can handle the load).  \n",
    "\n",
    "\n",
    "Kumar, S. (2023, July 18). Deep Q-Learning. GeeksforGeeks. https://www.geeksforgeeks.org/deep-q-learning/\n",
    "\n",
    "Hugging Face. (n.d.). Deep Q-Algorithm. Hugging Face. https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
